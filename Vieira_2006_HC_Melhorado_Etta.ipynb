{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vieira_2006_HC_Melhorado_Etta.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNkvBqyDHzXO9SW/xbauVMe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muriloufu/Hydrocyclone_ANN/blob/main/Vieira_2006_HC_Melhorado_Etta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOID4YHpNewq"
      },
      "source": [
        "#bibliotecas\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "from google.colab import files\n",
        "import numpy as np\n",
        "import io\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from google.colab import files\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "%matplotlib inline"
      ],
      "execution_count": 603,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tHeAwtRNnC5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 824
        },
        "outputId": "cb5a3020-5f3f-449e-d2e7-d72fdafd9e86"
      },
      "source": [
        "#selecao do dataset\n",
        "path = 'https://github.com/muriloufu/Hydrocyclone_ANN/raw/main/Tese_LG_2006_01.xlsx'\n",
        "df = pd.read_excel(path)\n",
        "df"
      ],
      "execution_count": 604,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Di</th>\n",
              "      <th>Do</th>\n",
              "      <th>L</th>\n",
              "      <th>Teta</th>\n",
              "      <th>RL</th>\n",
              "      <th>Eu</th>\n",
              "      <th>Etta</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.8</td>\n",
              "      <td>6.6</td>\n",
              "      <td>142</td>\n",
              "      <td>11.2</td>\n",
              "      <td>28.28</td>\n",
              "      <td>4493</td>\n",
              "      <td>65.18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.8</td>\n",
              "      <td>6.6</td>\n",
              "      <td>142</td>\n",
              "      <td>17.8</td>\n",
              "      <td>29.30</td>\n",
              "      <td>4407</td>\n",
              "      <td>61.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.8</td>\n",
              "      <td>6.6</td>\n",
              "      <td>207</td>\n",
              "      <td>11.2</td>\n",
              "      <td>31.80</td>\n",
              "      <td>3997</td>\n",
              "      <td>72.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.8</td>\n",
              "      <td>6.6</td>\n",
              "      <td>207</td>\n",
              "      <td>17.8</td>\n",
              "      <td>31.20</td>\n",
              "      <td>3664</td>\n",
              "      <td>62.23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.8</td>\n",
              "      <td>9.6</td>\n",
              "      <td>142</td>\n",
              "      <td>11.2</td>\n",
              "      <td>10.24</td>\n",
              "      <td>3697</td>\n",
              "      <td>54.06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>4.8</td>\n",
              "      <td>9.6</td>\n",
              "      <td>142</td>\n",
              "      <td>17.8</td>\n",
              "      <td>10.50</td>\n",
              "      <td>3211</td>\n",
              "      <td>47.30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>4.8</td>\n",
              "      <td>9.6</td>\n",
              "      <td>207</td>\n",
              "      <td>11.2</td>\n",
              "      <td>15.04</td>\n",
              "      <td>3257</td>\n",
              "      <td>61.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>4.8</td>\n",
              "      <td>9.6</td>\n",
              "      <td>207</td>\n",
              "      <td>17.8</td>\n",
              "      <td>17.16</td>\n",
              "      <td>3016</td>\n",
              "      <td>54.30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>7.8</td>\n",
              "      <td>6.6</td>\n",
              "      <td>142</td>\n",
              "      <td>11.2</td>\n",
              "      <td>29.00</td>\n",
              "      <td>2416</td>\n",
              "      <td>74.59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>7.8</td>\n",
              "      <td>6.6</td>\n",
              "      <td>142</td>\n",
              "      <td>17.8</td>\n",
              "      <td>33.00</td>\n",
              "      <td>1789</td>\n",
              "      <td>69.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>7.8</td>\n",
              "      <td>6.6</td>\n",
              "      <td>207</td>\n",
              "      <td>11.2</td>\n",
              "      <td>32.68</td>\n",
              "      <td>1587</td>\n",
              "      <td>72.32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>7.8</td>\n",
              "      <td>6.6</td>\n",
              "      <td>207</td>\n",
              "      <td>17.8</td>\n",
              "      <td>31.72</td>\n",
              "      <td>1467</td>\n",
              "      <td>63.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>7.8</td>\n",
              "      <td>9.6</td>\n",
              "      <td>142</td>\n",
              "      <td>11.2</td>\n",
              "      <td>11.00</td>\n",
              "      <td>1679</td>\n",
              "      <td>68.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>7.8</td>\n",
              "      <td>9.6</td>\n",
              "      <td>142</td>\n",
              "      <td>17.8</td>\n",
              "      <td>14.10</td>\n",
              "      <td>1177</td>\n",
              "      <td>59.83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>7.8</td>\n",
              "      <td>9.6</td>\n",
              "      <td>207</td>\n",
              "      <td>11.2</td>\n",
              "      <td>16.54</td>\n",
              "      <td>1071</td>\n",
              "      <td>62.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>7.8</td>\n",
              "      <td>9.6</td>\n",
              "      <td>207</td>\n",
              "      <td>17.8</td>\n",
              "      <td>16.19</td>\n",
              "      <td>1001</td>\n",
              "      <td>56.44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>3.9</td>\n",
              "      <td>8.1</td>\n",
              "      <td>174</td>\n",
              "      <td>14.5</td>\n",
              "      <td>15.61</td>\n",
              "      <td>6435</td>\n",
              "      <td>62.84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>8.7</td>\n",
              "      <td>8.1</td>\n",
              "      <td>174</td>\n",
              "      <td>14.5</td>\n",
              "      <td>18.90</td>\n",
              "      <td>1109</td>\n",
              "      <td>60.35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>6.3</td>\n",
              "      <td>5.7</td>\n",
              "      <td>174</td>\n",
              "      <td>14.5</td>\n",
              "      <td>42.54</td>\n",
              "      <td>2866</td>\n",
              "      <td>76.73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>6.3</td>\n",
              "      <td>10.5</td>\n",
              "      <td>174</td>\n",
              "      <td>14.5</td>\n",
              "      <td>12.43</td>\n",
              "      <td>1671</td>\n",
              "      <td>66.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>6.3</td>\n",
              "      <td>8.1</td>\n",
              "      <td>117</td>\n",
              "      <td>14.5</td>\n",
              "      <td>15.65</td>\n",
              "      <td>2744</td>\n",
              "      <td>55.61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>6.3</td>\n",
              "      <td>8.1</td>\n",
              "      <td>228</td>\n",
              "      <td>14.5</td>\n",
              "      <td>20.11</td>\n",
              "      <td>1525</td>\n",
              "      <td>49.17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>6.3</td>\n",
              "      <td>8.1</td>\n",
              "      <td>174</td>\n",
              "      <td>9.0</td>\n",
              "      <td>20.22</td>\n",
              "      <td>2245</td>\n",
              "      <td>63.44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>6.3</td>\n",
              "      <td>8.1</td>\n",
              "      <td>174</td>\n",
              "      <td>20.0</td>\n",
              "      <td>16.96</td>\n",
              "      <td>1801</td>\n",
              "      <td>61.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>6.3</td>\n",
              "      <td>8.1</td>\n",
              "      <td>174</td>\n",
              "      <td>14.5</td>\n",
              "      <td>17.50</td>\n",
              "      <td>1904</td>\n",
              "      <td>62.74</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Di    Do    L  Teta     RL    Eu   Etta\n",
              "0   4.8   6.6  142  11.2  28.28  4493  65.18\n",
              "1   4.8   6.6  142  17.8  29.30  4407  61.76\n",
              "2   4.8   6.6  207  11.2  31.80  3997  72.47\n",
              "3   4.8   6.6  207  17.8  31.20  3664  62.23\n",
              "4   4.8   9.6  142  11.2  10.24  3697  54.06\n",
              "5   4.8   9.6  142  17.8  10.50  3211  47.30\n",
              "6   4.8   9.6  207  11.2  15.04  3257  61.52\n",
              "7   4.8   9.6  207  17.8  17.16  3016  54.30\n",
              "8   7.8   6.6  142  11.2  29.00  2416  74.59\n",
              "9   7.8   6.6  142  17.8  33.00  1789  69.00\n",
              "10  7.8   6.6  207  11.2  32.68  1587  72.32\n",
              "11  7.8   6.6  207  17.8  31.72  1467  63.40\n",
              "12  7.8   9.6  142  11.2  11.00  1679  68.82\n",
              "13  7.8   9.6  142  17.8  14.10  1177  59.83\n",
              "14  7.8   9.6  207  11.2  16.54  1071  62.42\n",
              "15  7.8   9.6  207  17.8  16.19  1001  56.44\n",
              "16  3.9   8.1  174  14.5  15.61  6435  62.84\n",
              "17  8.7   8.1  174  14.5  18.90  1109  60.35\n",
              "18  6.3   5.7  174  14.5  42.54  2866  76.73\n",
              "19  6.3  10.5  174  14.5  12.43  1671  66.00\n",
              "20  6.3   8.1  117  14.5  15.65  2744  55.61\n",
              "21  6.3   8.1  228  14.5  20.11  1525  49.17\n",
              "22  6.3   8.1  174   9.0  20.22  2245  63.44\n",
              "23  6.3   8.1  174  20.0  16.96  1801  61.14\n",
              "24  6.3   8.1  174  14.5  17.50  1904  62.74"
            ]
          },
          "metadata": {},
          "execution_count": 604
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pX_18br6NreR"
      },
      "source": [
        "# funcao normatiza dados\n",
        "def Normatiza(x):\n",
        "    strings=list(x)\n",
        "    for i in strings:\n",
        "        max_x=x[i].max()\n",
        "        min_x=x[i].min()\n",
        "        x[i]=2*((x[i]-min_x)/(max_x-min_x))-1\n",
        "    return x"
      ],
      "execution_count": 605,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6NFycRWNtEI"
      },
      "source": [
        "# funcao retorna os dados a forma original - xi ISOLADO DA FUNCAO: NORMATIZA(X)\n",
        "def Original(x,x_old):\n",
        "    strings=list(x)\n",
        "    for i in strings:\n",
        "        max_x=x_old[i].max()\n",
        "        min_x=x_old[i].min()\n",
        "        x[i]=((x[i]+1)/2)*(max_x-min_x)+min_x\n",
        "    return x"
      ],
      "execution_count": 606,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aRm1LcrNuMk"
      },
      "source": [
        "DAT_OLD=[] #Criar um dataframe em branco\n",
        "DAT_OLD=df.copy() #No dataframe em branco esta colocando os dados de df (dataframe que chamou no inicio)\n",
        "DAT=Normatiza(df) #No dataframe DAT coloca o dataframe df normatizado"
      ],
      "execution_count": 607,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 824
        },
        "id": "iWlBoKYCCTZD",
        "outputId": "f46864eb-9ba8-4a7f-b9c0-8396ceb7e233"
      },
      "source": [
        "DAT #mostra o dataframe DAT que é o df normatizado"
      ],
      "execution_count": 608,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Di</th>\n",
              "      <th>Do</th>\n",
              "      <th>L</th>\n",
              "      <th>Teta</th>\n",
              "      <th>RL</th>\n",
              "      <th>Eu</th>\n",
              "      <th>Etta</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-6.250000e-01</td>\n",
              "      <td>-6.250000e-01</td>\n",
              "      <td>-0.549550</td>\n",
              "      <td>-0.6</td>\n",
              "      <td>0.117028</td>\n",
              "      <td>0.285241</td>\n",
              "      <td>0.215087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-6.250000e-01</td>\n",
              "      <td>-6.250000e-01</td>\n",
              "      <td>-0.549550</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.180186</td>\n",
              "      <td>0.253589</td>\n",
              "      <td>-0.017329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-6.250000e-01</td>\n",
              "      <td>-6.250000e-01</td>\n",
              "      <td>0.621622</td>\n",
              "      <td>-0.6</td>\n",
              "      <td>0.334985</td>\n",
              "      <td>0.102687</td>\n",
              "      <td>0.710499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-6.250000e-01</td>\n",
              "      <td>-6.250000e-01</td>\n",
              "      <td>0.621622</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.297833</td>\n",
              "      <td>-0.019875</td>\n",
              "      <td>0.014611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-6.250000e-01</td>\n",
              "      <td>6.250000e-01</td>\n",
              "      <td>-0.549550</td>\n",
              "      <td>-0.6</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.007729</td>\n",
              "      <td>-0.540605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-6.250000e-01</td>\n",
              "      <td>6.250000e-01</td>\n",
              "      <td>-0.549550</td>\n",
              "      <td>0.6</td>\n",
              "      <td>-0.983901</td>\n",
              "      <td>-0.186603</td>\n",
              "      <td>-1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-6.250000e-01</td>\n",
              "      <td>6.250000e-01</td>\n",
              "      <td>0.621622</td>\n",
              "      <td>-0.6</td>\n",
              "      <td>-0.702786</td>\n",
              "      <td>-0.169672</td>\n",
              "      <td>-0.033639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>-6.250000e-01</td>\n",
              "      <td>6.250000e-01</td>\n",
              "      <td>0.621622</td>\n",
              "      <td>0.6</td>\n",
              "      <td>-0.571517</td>\n",
              "      <td>-0.258373</td>\n",
              "      <td>-0.524295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>6.250000e-01</td>\n",
              "      <td>-6.250000e-01</td>\n",
              "      <td>-0.549550</td>\n",
              "      <td>-0.6</td>\n",
              "      <td>0.161610</td>\n",
              "      <td>-0.479205</td>\n",
              "      <td>0.854570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>6.250000e-01</td>\n",
              "      <td>-6.250000e-01</td>\n",
              "      <td>-0.549550</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.409288</td>\n",
              "      <td>-0.709974</td>\n",
              "      <td>0.474686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>6.250000e-01</td>\n",
              "      <td>-6.250000e-01</td>\n",
              "      <td>0.621622</td>\n",
              "      <td>-0.6</td>\n",
              "      <td>0.389474</td>\n",
              "      <td>-0.784321</td>\n",
              "      <td>0.700306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>6.250000e-01</td>\n",
              "      <td>-6.250000e-01</td>\n",
              "      <td>0.621622</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.330031</td>\n",
              "      <td>-0.828487</td>\n",
              "      <td>0.094122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>6.250000e-01</td>\n",
              "      <td>6.250000e-01</td>\n",
              "      <td>-0.549550</td>\n",
              "      <td>-0.6</td>\n",
              "      <td>-0.952941</td>\n",
              "      <td>-0.750460</td>\n",
              "      <td>0.462453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>6.250000e-01</td>\n",
              "      <td>6.250000e-01</td>\n",
              "      <td>-0.549550</td>\n",
              "      <td>0.6</td>\n",
              "      <td>-0.760991</td>\n",
              "      <td>-0.935223</td>\n",
              "      <td>-0.148488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>6.250000e-01</td>\n",
              "      <td>6.250000e-01</td>\n",
              "      <td>0.621622</td>\n",
              "      <td>-0.6</td>\n",
              "      <td>-0.609907</td>\n",
              "      <td>-0.974236</td>\n",
              "      <td>0.027523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>6.250000e-01</td>\n",
              "      <td>6.250000e-01</td>\n",
              "      <td>0.621622</td>\n",
              "      <td>0.6</td>\n",
              "      <td>-0.631579</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>-0.378865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>-1.000000e+00</td>\n",
              "      <td>-2.220446e-16</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.667492</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.056065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>-2.220446e-16</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.463777</td>\n",
              "      <td>-0.960250</td>\n",
              "      <td>-0.113150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2.220446e-16</td>\n",
              "      <td>-1.000000e+00</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.313581</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2.220446e-16</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.864396</td>\n",
              "      <td>-0.753404</td>\n",
              "      <td>0.270812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>2.220446e-16</td>\n",
              "      <td>-2.220446e-16</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.665015</td>\n",
              "      <td>-0.358484</td>\n",
              "      <td>-0.435270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>2.220446e-16</td>\n",
              "      <td>-2.220446e-16</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.388854</td>\n",
              "      <td>-0.807140</td>\n",
              "      <td>-0.872919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>2.220446e-16</td>\n",
              "      <td>-2.220446e-16</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-0.382043</td>\n",
              "      <td>-0.542142</td>\n",
              "      <td>0.096840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>2.220446e-16</td>\n",
              "      <td>-2.220446e-16</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.583901</td>\n",
              "      <td>-0.705558</td>\n",
              "      <td>-0.059463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>2.220446e-16</td>\n",
              "      <td>-2.220446e-16</td>\n",
              "      <td>0.027027</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.550464</td>\n",
              "      <td>-0.667648</td>\n",
              "      <td>0.049269</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Di            Do         L  Teta        RL        Eu      Etta\n",
              "0  -6.250000e-01 -6.250000e-01 -0.549550  -0.6  0.117028  0.285241  0.215087\n",
              "1  -6.250000e-01 -6.250000e-01 -0.549550   0.6  0.180186  0.253589 -0.017329\n",
              "2  -6.250000e-01 -6.250000e-01  0.621622  -0.6  0.334985  0.102687  0.710499\n",
              "3  -6.250000e-01 -6.250000e-01  0.621622   0.6  0.297833 -0.019875  0.014611\n",
              "4  -6.250000e-01  6.250000e-01 -0.549550  -0.6 -1.000000 -0.007729 -0.540605\n",
              "5  -6.250000e-01  6.250000e-01 -0.549550   0.6 -0.983901 -0.186603 -1.000000\n",
              "6  -6.250000e-01  6.250000e-01  0.621622  -0.6 -0.702786 -0.169672 -0.033639\n",
              "7  -6.250000e-01  6.250000e-01  0.621622   0.6 -0.571517 -0.258373 -0.524295\n",
              "8   6.250000e-01 -6.250000e-01 -0.549550  -0.6  0.161610 -0.479205  0.854570\n",
              "9   6.250000e-01 -6.250000e-01 -0.549550   0.6  0.409288 -0.709974  0.474686\n",
              "10  6.250000e-01 -6.250000e-01  0.621622  -0.6  0.389474 -0.784321  0.700306\n",
              "11  6.250000e-01 -6.250000e-01  0.621622   0.6  0.330031 -0.828487  0.094122\n",
              "12  6.250000e-01  6.250000e-01 -0.549550  -0.6 -0.952941 -0.750460  0.462453\n",
              "13  6.250000e-01  6.250000e-01 -0.549550   0.6 -0.760991 -0.935223 -0.148488\n",
              "14  6.250000e-01  6.250000e-01  0.621622  -0.6 -0.609907 -0.974236  0.027523\n",
              "15  6.250000e-01  6.250000e-01  0.621622   0.6 -0.631579 -1.000000 -0.378865\n",
              "16 -1.000000e+00 -2.220446e-16  0.027027   0.0 -0.667492  1.000000  0.056065\n",
              "17  1.000000e+00 -2.220446e-16  0.027027   0.0 -0.463777 -0.960250 -0.113150\n",
              "18  2.220446e-16 -1.000000e+00  0.027027   0.0  1.000000 -0.313581  1.000000\n",
              "19  2.220446e-16  1.000000e+00  0.027027   0.0 -0.864396 -0.753404  0.270812\n",
              "20  2.220446e-16 -2.220446e-16 -1.000000   0.0 -0.665015 -0.358484 -0.435270\n",
              "21  2.220446e-16 -2.220446e-16  1.000000   0.0 -0.388854 -0.807140 -0.872919\n",
              "22  2.220446e-16 -2.220446e-16  0.027027  -1.0 -0.382043 -0.542142  0.096840\n",
              "23  2.220446e-16 -2.220446e-16  0.027027   1.0 -0.583901 -0.705558 -0.059463\n",
              "24  2.220446e-16 -2.220446e-16  0.027027   0.0 -0.550464 -0.667648  0.049269"
            ]
          },
          "metadata": {},
          "execution_count": 608
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHAmeF3u9b1v"
      },
      "source": [
        "#dividindo randomicamente os dados de DAT em treino e teste\n",
        "train=DAT.iloc[[0,1,2,3,5,6,7,8,9,10,11,12,14,15,16,17,20,21,22,23],:]\n",
        "test=DAT.iloc[[4,13,18,19,24],:]\n",
        "#train=DAT.sample(frac=0.8,random_state=None)\n",
        "#test=DAT.drop(train.index)"
      ],
      "execution_count": 609,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hb3NrCwW9mUU"
      },
      "source": [
        "#Definindo as variaveis independentes\n",
        "x_train=train.iloc[:,[0,1,2,3]]\n",
        "x_test=test.iloc[:,[0,1,2,3]]\n",
        "X_OLD=DAT_OLD.iloc[:,[0,1,2,3]]"
      ],
      "execution_count": 610,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhHep1do94DF",
        "outputId": "932dfcca-ba14-4a04-e405-fa6dd4c33ccc"
      },
      "source": [
        "#variaveis independentes de treino e teste\n",
        "print(x_train)\n",
        "print (x_test)"
      ],
      "execution_count": 611,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              Di            Do         L  Teta\n",
            "0  -6.250000e-01 -6.250000e-01 -0.549550  -0.6\n",
            "1  -6.250000e-01 -6.250000e-01 -0.549550   0.6\n",
            "2  -6.250000e-01 -6.250000e-01  0.621622  -0.6\n",
            "3  -6.250000e-01 -6.250000e-01  0.621622   0.6\n",
            "5  -6.250000e-01  6.250000e-01 -0.549550   0.6\n",
            "6  -6.250000e-01  6.250000e-01  0.621622  -0.6\n",
            "7  -6.250000e-01  6.250000e-01  0.621622   0.6\n",
            "8   6.250000e-01 -6.250000e-01 -0.549550  -0.6\n",
            "9   6.250000e-01 -6.250000e-01 -0.549550   0.6\n",
            "10  6.250000e-01 -6.250000e-01  0.621622  -0.6\n",
            "11  6.250000e-01 -6.250000e-01  0.621622   0.6\n",
            "12  6.250000e-01  6.250000e-01 -0.549550  -0.6\n",
            "14  6.250000e-01  6.250000e-01  0.621622  -0.6\n",
            "15  6.250000e-01  6.250000e-01  0.621622   0.6\n",
            "16 -1.000000e+00 -2.220446e-16  0.027027   0.0\n",
            "17  1.000000e+00 -2.220446e-16  0.027027   0.0\n",
            "20  2.220446e-16 -2.220446e-16 -1.000000   0.0\n",
            "21  2.220446e-16 -2.220446e-16  1.000000   0.0\n",
            "22  2.220446e-16 -2.220446e-16  0.027027  -1.0\n",
            "23  2.220446e-16 -2.220446e-16  0.027027   1.0\n",
            "              Di            Do         L  Teta\n",
            "4  -6.250000e-01  6.250000e-01 -0.549550  -0.6\n",
            "13  6.250000e-01  6.250000e-01 -0.549550   0.6\n",
            "18  2.220446e-16 -1.000000e+00  0.027027   0.0\n",
            "19  2.220446e-16  1.000000e+00  0.027027   0.0\n",
            "24  2.220446e-16 -2.220446e-16  0.027027   0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JV4vJ0_69yiE",
        "outputId": "6de27ff5-82bb-4333-9f1c-798263da7d70"
      },
      "source": [
        "#definindo a variavel dependente Etta\n",
        "y_train=train.iloc[:,[6]]\n",
        "y_test=test.iloc[:,[6]]\n",
        "Y_OLD=DAT_OLD.iloc[:,[6]]\n",
        "print(y_train)\n",
        "print(y_test)"
      ],
      "execution_count": 612,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Etta\n",
            "0   0.215087\n",
            "1  -0.017329\n",
            "2   0.710499\n",
            "3   0.014611\n",
            "5  -1.000000\n",
            "6  -0.033639\n",
            "7  -0.524295\n",
            "8   0.854570\n",
            "9   0.474686\n",
            "10  0.700306\n",
            "11  0.094122\n",
            "12  0.462453\n",
            "14  0.027523\n",
            "15 -0.378865\n",
            "16  0.056065\n",
            "17 -0.113150\n",
            "20 -0.435270\n",
            "21 -0.872919\n",
            "22  0.096840\n",
            "23 -0.059463\n",
            "        Etta\n",
            "4  -0.540605\n",
            "13 -0.148488\n",
            "18  1.000000\n",
            "19  0.270812\n",
            "24  0.049269\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4haSAcEPN2S7"
      },
      "source": [
        "#define a rede neural\n",
        "#nessa rede vao ser usados 3 camadas: Input + hidden + output\n",
        "#o numero de neuronios da output eh 1, porque tem 1 resposta (FIE)\n",
        "#o numero de neuronios na hidden eh arbitrario. O artigo fez 12 neuronios nessa camada\n",
        "#uncao de ativicao eh a logistic, segundo o artigo\n",
        "model = MLPRegressor(random_state=1,solver='lbfgs',activation='tanh', learning_rate = 'adaptive', alpha=1e-5, \n",
        "                     hidden_layer_sizes= tuple(12 for _ in range(1)))\n",
        "\n"
      ],
      "execution_count": 613,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAhi90gVN3YV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baaf4abf-7629-4533-e82e-5daa91b38324"
      },
      "source": [
        "#treina a rede neural\n",
        "model.fit(x_train, y_train)"
      ],
      "execution_count": 614,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:1342: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPRegressor(activation='tanh', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
              "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "             hidden_layer_sizes=(12,), learning_rate='adaptive',\n",
              "             learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
              "             momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "             power_t=0.5, random_state=1, shuffle=True, solver='lbfgs',\n",
              "             tol=0.0001, validation_fraction=0.1, verbose=False,\n",
              "             warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 614
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-yOW0ZmN4e0"
      },
      "source": [
        "#usa a rede neural para treino e teste\n",
        "y_calc_train=model.predict(x_train)\n",
        "y_calc_test=model.predict(x_test)"
      ],
      "execution_count": 615,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tfwjcMtN58a"
      },
      "source": [
        "#transforma a saida da rede em dataframe \n",
        "y_calc_train=pd.DataFrame(y_calc_train)\n",
        "y_calc_test=pd.DataFrame(y_calc_test)\n",
        "col_names=list(y_train)\n",
        "y_calc_train.columns = col_names\n",
        "y_calc_test.columns = col_names"
      ],
      "execution_count": 616,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RboALKIN7as"
      },
      "source": [
        "#Desnormatizar os dados obtidos da rede neural\n",
        "y_calc_train=Original(y_calc_train,Y_OLD)\n",
        "y_calc_test=Original(y_calc_test,Y_OLD)"
      ],
      "execution_count": 617,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjoCPU8bN92O"
      },
      "source": [
        "#recupera os dados originais \n",
        "test=[]\n",
        "train=[]\n",
        "train=Original(y_train,Y_OLD)\n",
        "test=Original(y_test,Y_OLD)"
      ],
      "execution_count": 618,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiL9upNxN_RT"
      },
      "source": [
        "#Resposta para fazer o grafico (Etta-treino)\n",
        "resist_obs_train=[]\n",
        "resist_calc_train=[]\n",
        "resist_calc_train=y_calc_train['Etta'].copy()\n",
        "resist_obs_train=train['Etta'].copy()"
      ],
      "execution_count": 619,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99_g9O8DOAfQ"
      },
      "source": [
        "#Resposta para fazer o grafico (Etta-teste)\n",
        "resist_obs_test=[]\n",
        "resist_calc_test=[]\n",
        "resist_calc_test=y_calc_test['Etta'].copy()\n",
        "resist_obs_test=test['Etta'].copy()"
      ],
      "execution_count": 620,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRHCXQdoOBtT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "cc43e1c5-2ff5-405d-cd51-fe924fca5ad2"
      },
      "source": [
        "#expressa os dois dados em figura - ETTA\n",
        "fig = plt.figure()\n",
        "ax1 = fig.add_subplot(111)\n",
        "\n",
        "ax1.scatter(resist_obs_test,resist_calc_test, s=10, c='b', marker=\"s\", label='teste')\n",
        "ax1.scatter(resist_obs_train,resist_calc_train, s=10, c='r', marker=\"o\", label='treino')\n",
        "plt.legend(loc='upper left')\n",
        "plt.ylabel('Etta Calculado')\n",
        "plt.xlabel('Etta Observado')\n",
        "plt.show()"
      ],
      "execution_count": 621,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxqGLrm6OCHC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e014a10-ccb1-4230-e811-8790a7eaeeb5"
      },
      "source": [
        "#checar a qualidade da regressao PARA TESTE\n",
        "mse=mean_squared_error(resist_obs_test,resist_calc_test)\n",
        "print(\"MSE teste=\",mse)\n",
        "R2=r2_score(resist_obs_test,resist_calc_test)\n",
        "print(\"R^2 teste=\",R2)"
      ],
      "execution_count": 622,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE teste= 0.48517900721749596\n",
            "R^2 teste= 0.9914506227215506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEznkoupODnN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "960592f0-84fd-4d01-8798-9a5f5251ddac"
      },
      "source": [
        "#checar a qualidade da regressao PARA TREINO\n",
        "mse=mean_squared_error(resist_obs_train,resist_calc_train)\n",
        "print(\"MSE treino=\",mse)\n",
        "R2=r2_score(resist_obs_train,resist_calc_train)\n",
        "print(\"R^2 treino=\",R2)"
      ],
      "execution_count": 623,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE treino= 0.0002508906419418864\n",
            "R^2 treino= 0.9999949811075974\n"
          ]
        }
      ]
    }
  ]
}